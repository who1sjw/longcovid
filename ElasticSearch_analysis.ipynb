{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7479f3-934a-4896-8aac-4c0bf8634420",
   "metadata": {},
   "source": [
    "# 1. Elasticsearch: Create twitter_covid index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84e2c842-9f0f-4d3b-a896-d14c8c4a3748",
   "metadata": {},
   "source": [
    "PUT /twitter_covid\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"user_name\": { \"type\": \"keyword\" },\n",
    "      \"user_location\": { \"type\": \"text\" },\n",
    "      \"user_description\": { \"type\": \"text\" },\n",
    "      \"user_created\": { \"type\": \"date\" },\n",
    "      \"user_followers\": { \"type\": \"integer\" },\n",
    "      \"user_friends\": { \"type\": \"integer\" },\n",
    "      \"user_favourites\": { \"type\": \"integer\" },\n",
    "      \"user_verified\": { \"type\": \"boolean\" },\n",
    "      \"date\": { \"type\": \"date\" },\n",
    "      \"text\": { \"type\": \"text\" },\n",
    "      \"hashtags\": { \"type\": \"keyword\" },\n",
    "      \"source\": { \"type\": \"keyword\" },\n",
    "      \"is_retweet\": { \"type\": \"boolean\" }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6ad30-c66d-47fe-9b58-c0239b99626b",
   "metadata": {},
   "source": [
    "# 2. Elasticsearch: Create twitter_covid index pattern"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f37ad8f-864f-4e21-b684-6857a3d2315a",
   "metadata": {},
   "source": [
    "curl -X POST \"http://localhost:5601/api/saved_objects/index-pattern\" \\\n",
    "     -H \"kbn-xsrf: true\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"attributes\": {\"title\": \"twitter_covid*\", \"timeFieldName\": \"date\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443681f-e6b6-496c-bbaf-055e933f89e7",
   "metadata": {},
   "source": [
    "# 3. import preprocessed data to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30a61d6e-32b9-4786-b51b-5145477d7cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data import completed\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "# Date format conversion function\n",
    "def convert_date(date_string):\n",
    "    return datetime.strptime(date_string, '%Y-%m-%d %H:%M:%S').isoformat()\n",
    "\n",
    "# Safely parse hashtags\n",
    "def parse_hashtags(hashtags_string):\n",
    "    try:\n",
    "        # Attempt to safely parse the string using ast.literal_eval\n",
    "        return ast.literal_eval(hashtags_string)\n",
    "    except:\n",
    "        # If parsing fails, return an empty list\n",
    "        return []\n",
    "\n",
    "# Open and read the CSV file\n",
    "with open('covid19_tweets.csv', 'r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        # Convert date fields\n",
    "        row['user_created'] = convert_date(row['user_created'])\n",
    "        row['date'] = convert_date(row['date'])\n",
    "        \n",
    "        # Convert numerical fields\n",
    "        row['user_followers'] = int(row['user_followers'])\n",
    "        row['user_friends'] = int(row['user_friends'])\n",
    "        row['user_favourites'] = int(row['user_favourites'])\n",
    "        \n",
    "        # Convert boolean fields\n",
    "        row['user_verified'] = row['user_verified'].lower() == 'true'\n",
    "        row['is_retweet'] = row['is_retweet'].lower() == 'true'\n",
    "        \n",
    "        # Safely handle the hashtags field\n",
    "        row['hashtags'] = parse_hashtags(row['hashtags'])\n",
    "        \n",
    "        # Use user_name as the document ID\n",
    "        doc_id = row['user_name']\n",
    "        \n",
    "        # Index the row data as a document into Elasticsearch\n",
    "        es.index(index='twitter_covid', id=doc_id, body=row)\n",
    "\n",
    "print(\"Data import completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8c147-82c0-4180-83f8-5a316269cf46",
   "metadata": {},
   "source": [
    "# 4. Analyse ES Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018ab14-fbb7-4fc0-b8fd-8afc6f25c0f9",
   "metadata": {},
   "source": [
    "## 4.1 Geographic Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11bf92d0-e1b0-4704-9cd4-692911b78c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import folium\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "def get_data_from_es(index_name, size=10000):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    response = es.search(index=index_name, body=query)\n",
    "    return [hit['_source'] for hit in response['hits']['hits']]\n",
    "\n",
    "# Fetch data\n",
    "data = get_data_from_es('twitter_covid')\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "353ebb54-a63f-4f7c-b19b-fd0524a37db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 locations:\n",
      "united states: 135\n",
      "india: 120\n",
      "london, england: 107\n",
      "united kingdom: 91\n",
      "lagos, nigeria: 87\n",
      "london: 78\n",
      "new delhi, india: 72\n",
      "australia: 55\n",
      "nairobi, kenya: 53\n",
      "usa: 53\n",
      "nigeria: 52\n",
      "south africa: 50\n",
      "uk: 50\n",
      "new york, ny: 49\n",
      "johannesburg, south africa: 44\n",
      "canada: 41\n",
      "chicago, il: 41\n",
      "los angeles, ca: 41\n",
      "mumbai, india: 38\n",
      "washington, dc: 36\n",
      "\n",
      "Unique locations saved to 'unique_locations.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "from collections import Counter\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "def get_data_from_es(index_name, size=10000):\n",
    "    \"\"\"\n",
    "    Fetch data from Elasticsearch\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    response = es.search(index=index_name, body=query)\n",
    "    return [hit['_source'] for hit in response['hits']['hits']]\n",
    "\n",
    "# Get data\n",
    "data = get_data_from_es('twitter_covid')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def clean_location(location):\n",
    "    \"\"\"\n",
    "    Clean location data by removing special characters and standardizing format\n",
    "    \"\"\"\n",
    "    if pd.isna(location) or not isinstance(location, str):\n",
    "        return None\n",
    "    \n",
    "    # Convert to lowercase and remove special characters\n",
    "    location = re.sub(r'[^\\w\\s,]', '', location.lower()).strip()\n",
    "    \n",
    "    # Remove common prefixes/suffixes that don't add geographic information\n",
    "    prefixes_suffixes = ['the', 'a', 'an', 'in', 'at', 'on', 'from']\n",
    "    location_parts = location.split()\n",
    "    location_parts = [part for part in location_parts if part not in prefixes_suffixes]\n",
    "    location = ' '.join(location_parts)\n",
    "    \n",
    "    # Remove very short or likely non-geographic locations\n",
    "    if len(location) < 2 or location in ['earth', 'world', 'global', 'international']:\n",
    "        return None\n",
    "    \n",
    "    return location\n",
    "\n",
    "# Apply the cleaning function to the location column\n",
    "df['cleaned_location'] = df['user_location'].apply(clean_location)\n",
    "\n",
    "# Remove rows with None values in cleaned_location\n",
    "df = df.dropna(subset=['cleaned_location'])\n",
    "\n",
    "# Count occurrences of each unique location\n",
    "location_counts = Counter(df['cleaned_location'])\n",
    "\n",
    "# Get the top N locations\n",
    "def print_top_locations(n=20):\n",
    "    \"\"\"\n",
    "    Print the top N most common locations\n",
    "    \"\"\"\n",
    "    print(f\"Top {n} locations:\")\n",
    "    for location, count in location_counts.most_common(n):\n",
    "        print(f\"{location}: {count}\")\n",
    "\n",
    "# Print top 20 locations\n",
    "print_top_locations(20)\n",
    "\n",
    "# Optionally, save the cleaned and unique locations to a file\n",
    "unique_locations = pd.DataFrame(location_counts.items(), columns=['Location', 'Count'])\n",
    "unique_locations = unique_locations.sort_values('Count', ascending=False)\n",
    "unique_locations.to_csv('unique_locations.csv', index=False)\n",
    "print(\"\\nUnique locations saved to 'unique_locations.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0514dd0d-19f2-463d-b9dc-3b07674d1f11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 locations for long COVID related tweets:\n",
      "nyc: 2\n",
      "oslo, norway: 1\n",
      "wwwlaurienadelcom: 1\n",
      "barcelona: 1\n",
      "nc, usa: 1\n",
      "sigtuna, sverige: 1\n",
      "cheltenham, uk: 1\n",
      "menlo park sfo world: 1\n",
      "den haag, nederland: 1\n",
      "north america: 1\n",
      "bengaluru, india: 1\n",
      "west sussex: 1\n",
      "indonesia: 1\n",
      "university of leicester: 1\n",
      "cleveland ohio: 1\n",
      "los angeles, ca: 1\n",
      "everett, washington: 1\n",
      "fierce be with youalways: 1\n",
      "maryland metro dc: 1\n",
      "\n",
      "Unique locations for long COVID related tweets saved to 'long_covid_locations.csv'\n",
      "\n",
      "Total number of tweets with long COVID related hashtags: 20\n",
      "Number of unique locations: 19\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "def get_long_covid_data(index_name, size=10000):\n",
    "    \"\"\"\n",
    "    Fetch data from Elasticsearch with specified long COVID related hashtags\n",
    "    \"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\"match\": {\"hashtags\": \"longhaulers\"}},\n",
    "                    {\"match\": {\"hashtags\": \"longcovid\"}},\n",
    "                    {\"match\": {\"hashtags\": \"longhauler\"}},\n",
    "                    {\"match\": {\"hashtags\": \"longtermcare\"}},\n",
    "                    {\"match\": {\"hashtags\": \"longhaul\"}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    response = es.search(index=index_name, body=query)\n",
    "    return [hit['_source'] for hit in response['hits']['hits']]\n",
    "\n",
    "# Get long COVID related data\n",
    "data = get_long_covid_data('twitter_covid')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def clean_location(location):\n",
    "    \"\"\"\n",
    "    Clean location data by removing special characters and standardizing format\n",
    "    \"\"\"\n",
    "    if pd.isna(location) or not isinstance(location, str):\n",
    "        return None\n",
    "    \n",
    "    # Convert to lowercase and remove special characters\n",
    "    location = re.sub(r'[^\\w\\s,]', '', location.lower()).strip()\n",
    "    \n",
    "    # Remove common prefixes/suffixes that don't add geographic information\n",
    "    prefixes_suffixes = ['the', 'a', 'an', 'in', 'at', 'on', 'from']\n",
    "    location_parts = location.split()\n",
    "    location_parts = [part for part in location_parts if part not in prefixes_suffixes]\n",
    "    location = ' '.join(location_parts)\n",
    "    \n",
    "    # Remove very short or likely non-geographic locations\n",
    "    if len(location) < 2 or location in ['earth', 'world', 'global', 'international']:\n",
    "        return None\n",
    "    \n",
    "    return location\n",
    "\n",
    "# Apply the cleaning function to the location column\n",
    "df['cleaned_location'] = df['user_location'].apply(clean_location)\n",
    "\n",
    "# Remove rows with None values in cleaned_location\n",
    "df = df.dropna(subset=['cleaned_location'])\n",
    "\n",
    "# Count occurrences of each unique location\n",
    "location_counts = Counter(df['cleaned_location'])\n",
    "\n",
    "def print_top_locations(n=20):\n",
    "    \"\"\"\n",
    "    Print the top N most common locations\n",
    "    \"\"\"\n",
    "    print(f\"Top {n} locations for long COVID related tweets:\")\n",
    "    for location, count in location_counts.most_common(n):\n",
    "        print(f\"{location}: {count}\")\n",
    "\n",
    "# Print top 20 locations\n",
    "print_top_locations(20)\n",
    "\n",
    "# Save the cleaned and unique locations to a file\n",
    "unique_locations = pd.DataFrame(location_counts.items(), columns=['Location', 'Count'])\n",
    "unique_locations = unique_locations.sort_values('Count', ascending=False)\n",
    "unique_locations.to_csv('long_covid_locations.csv', index=False)\n",
    "print(\"\\nUnique locations for long COVID related tweets saved to 'long_covid_locations.csv'\")\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"\\nTotal number of tweets with long COVID related hashtags: {len(df)}\")\n",
    "print(f\"Number of unique locations: {len(location_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ccf9c11-ebf1-4669-9ae6-f24a0fce5990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved as 'covid_tweet_map.html'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load data from CSV files\n",
    "# all_locations = pd.read_csv('unique_locations.csv')\n",
    "# long_covid_locations = pd.read_csv('long_covid_locations.csv')\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "long_covid_data = pd.DataFrame([\n",
    "    ('New York City', 2), ('Oslo', 1), ('Barcelona', 1), ('North Carolina', 1),\n",
    "    ('Sigtuna', 1), ('Cheltenham', 1), ('Menlo Park', 1), ('The Hague', 1),\n",
    "    ('North America', 1), ('Bengaluru', 1), ('West Sussex', 1), ('Indonesia', 1),\n",
    "    ('Leicester', 1), ('Cleveland', 1), ('Los Angeles', 1), ('Everett', 1),\n",
    "    ('Maryland', 1)\n",
    "], columns=['Location', 'Count'])\n",
    "\n",
    "covid_data = pd.DataFrame([\n",
    "    ('United States', 135), ('India', 120), ('London', 185), ('United Kingdom', 141),\n",
    "    ('Lagos', 87), ('New Delhi', 72), ('Australia', 55), ('Nairobi', 53),\n",
    "    ('Nigeria', 52), ('South Africa', 94), ('New York', 49), ('Canada', 41),\n",
    "    ('Chicago', 41), ('Los Angeles', 41), ('Mumbai', 38), ('Washington DC', 36)\n",
    "], columns=['Location', 'Count'])\n",
    "\n",
    "# Location Dictionary\n",
    "coordinates = {\n",
    "    'New York City': (40.7128, -74.0060), 'Oslo': (59.9139, 10.7522),\n",
    "    'Barcelona': (41.3851, 2.1734), 'North Carolina': (35.7596, -79.0193),\n",
    "    'Sigtuna': (59.6173, 17.7231), 'Cheltenham': (51.8979, -2.0744),\n",
    "    'Menlo Park': (37.4538, -122.1822), 'The Hague': (52.0705, 4.3007),\n",
    "    'North America': (54.5260, -105.2551), 'Bengaluru': (12.9716, 77.5946),\n",
    "    'West Sussex': (50.9280, -0.4617), 'Indonesia': (-0.7893, 113.9213),\n",
    "    'Leicester': (52.6369, -1.1398), 'Cleveland': (41.4993, -81.6944),\n",
    "    'Los Angeles': (34.0522, -118.2437), 'Everett': (47.9790, -122.2021),\n",
    "    'Maryland': (39.0458, -76.6413), 'United States': (37.0902, -95.7129),\n",
    "    'India': (20.5937, 78.9629), 'London': (51.5074, -0.1278),\n",
    "    'United Kingdom': (55.3781, -3.4360), 'Lagos': (6.5244, 3.3792),\n",
    "    'New Delhi': (28.6139, 77.2090), 'Australia': (-25.2744, 133.7751),\n",
    "    'Nairobi': (-1.2921, 36.8219), 'Nigeria': (9.0820, 8.6753),\n",
    "    'South Africa': (-30.5595, 22.9375), 'New York': (40.7128, -74.0060),\n",
    "    'Canada': (56.1304, -106.3468), 'Chicago': (41.8781, -87.6298),\n",
    "    'Mumbai': (19.0760, 72.8777), 'Washington DC': (38.9072, -77.0369)\n",
    "}\n",
    "\n",
    "# Create Map\n",
    "m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "\n",
    "# Add COVID-19 Location\n",
    "covid_cluster = MarkerCluster(name=\"COVID-19 Tweets\").add_to(m)\n",
    "for _, row in covid_data.iterrows():\n",
    "    if row['Location'] in coordinates:\n",
    "        folium.CircleMarker(\n",
    "            location=coordinates[row['Location']],\n",
    "            radius=min(int(row['Count']/5), 20),\n",
    "            popup=f\"{row['Location']}: {row['Count']}\",\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_opacity=0.7\n",
    "        ).add_to(covid_cluster)\n",
    "\n",
    "# Add Long COVID Location\n",
    "for _, row in long_covid_data.iterrows():\n",
    "    if row['Location'] in coordinates:\n",
    "        folium.CircleMarker(\n",
    "            location=coordinates[row['Location']],\n",
    "            radius=10,  # 固定大小以突出显示\n",
    "            popup=f\"Long COVID - {row['Location']}: {row['Count']}\",\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_opacity=0.9,\n",
    "            weight=2\n",
    "        ).add_to(m)\n",
    "\n",
    "# Add tag\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; left: 50px; width: 220px; height: 90px; \n",
    "    border:2px solid grey; z-index:9999; font-size:14px; background-color:white;\n",
    "    \">&nbsp; <b>Legend</b> <br>\n",
    "    &nbsp; COVID-19 tweets &nbsp; <i class=\"fa fa-circle fa-1x\" style=\"color:blue\"></i><br>\n",
    "    &nbsp; Long COVID tweets &nbsp; <i class=\"fa fa-circle fa-1x\" style=\"color:red\"></i>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Add Layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Save map\n",
    "m.save('covid_tweet_map.html')\n",
    "\n",
    "print(\"Map saved as 'covid_tweet_map.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f18a3bb-ec27-4750-bf48-0927a9926c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved as 'covid_tweet_map.html'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Load data from CSV files\n",
    "# all_locations = pd.read_csv('unique_locations.csv')\n",
    "# long_covid_locations = pd.read_csv('long_covid_locations.csv')\n",
    "\n",
    "# Create dataframes for long COVID and general COVID data\n",
    "long_covid_data = pd.DataFrame([\n",
    "    ('New York City', 2), ('Oslo', 1), ('Barcelona', 1), ('North Carolina', 1),\n",
    "    ('Sigtuna', 1), ('Cheltenham', 1), ('Menlo Park', 1), ('The Hague', 1),\n",
    "    ('North America', 1), ('Bengaluru', 1), ('West Sussex', 1), ('Indonesia', 1),\n",
    "    ('Leicester', 1), ('Cleveland', 1), ('Los Angeles', 1), ('Everett', 1),\n",
    "    ('Maryland', 1)\n",
    "], columns=['Location', 'Count'])\n",
    "\n",
    "covid_data = pd.DataFrame([\n",
    "    ('United States', 135), ('India', 120), ('London', 185), ('United Kingdom', 141),\n",
    "    ('Lagos', 87), ('New Delhi', 72), ('Australia', 55), ('Nairobi', 53),\n",
    "    ('Nigeria', 52), ('South Africa', 94), ('New York', 49), ('Canada', 41),\n",
    "    ('Chicago', 41), ('Los Angeles', 41), ('Mumbai', 38), ('Washington DC', 36)\n",
    "], columns=['Location', 'Count'])\n",
    "\n",
    "# Dictionary of coordinates for each location\n",
    "coordinates = {\n",
    "    'New York City': (40.7128, -74.0060), 'Oslo': (59.9139, 10.7522),\n",
    "    'Barcelona': (41.3851, 2.1734), 'North Carolina': (35.7596, -79.0193),\n",
    "    'Sigtuna': (59.6173, 17.7231), 'Cheltenham': (51.8979, -2.0744),\n",
    "    'Menlo Park': (37.4538, -122.1822), 'The Hague': (52.0705, 4.3007),\n",
    "    'North America': (54.5260, -105.2551), 'Bengaluru': (12.9716, 77.5946),\n",
    "    'West Sussex': (50.9280, -0.4617), 'Indonesia': (-0.7893, 113.9213),\n",
    "    'Leicester': (52.6369, -1.1398), 'Cleveland': (41.4993, -81.6944),\n",
    "    'Los Angeles': (34.0522, -118.2437), 'Everett': (47.9790, -122.2021),\n",
    "    'Maryland': (39.0458, -76.6413), 'United States': (37.0902, -95.7129),\n",
    "    'India': (20.5937, 78.9629), 'London': (51.5074, -0.1278),\n",
    "    'United Kingdom': (55.3781, -3.4360), 'Lagos': (6.5244, 3.3792),\n",
    "    'New Delhi': (28.6139, 77.2090), 'Australia': (-25.2744, 133.7751),\n",
    "    'Nairobi': (-1.2921, 36.8219), 'Nigeria': (9.0820, 8.6753),\n",
    "    'South Africa': (-30.5595, 22.9375), 'New York': (40.7128, -74.0060),\n",
    "    'Canada': (56.1304, -106.3468), 'Chicago': (41.8781, -87.6298),\n",
    "    'Mumbai': (19.0760, 72.8777), 'Washington DC': (38.9072, -77.0369)\n",
    "}\n",
    "\n",
    "# Create map\n",
    "m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "\n",
    "# Function to create circle marker with count inside\n",
    "def create_circle_marker(location, count, color, radius):\n",
    "    folium.CircleMarker(\n",
    "        location=location,\n",
    "        radius=radius,\n",
    "        popup=f\"{location}: {count}\",\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(m)\n",
    "    folium.Marker(\n",
    "        location=location,\n",
    "        icon=folium.DivIcon(html=f\"\"\"\n",
    "            <div style=\"font-family: courier new; color: black\">\n",
    "            <strong>{count}</strong>\n",
    "            </div>\"\"\")\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add COVID-19 points\n",
    "for _, row in covid_data.iterrows():\n",
    "    if row['Location'] in coordinates:\n",
    "        create_circle_marker(\n",
    "            location=coordinates[row['Location']],\n",
    "            count=row['Count'],\n",
    "            color='blue',\n",
    "            radius=min(int(row['Count']/5), 20)\n",
    "        )\n",
    "\n",
    "# Add Long COVID points\n",
    "for _, row in long_covid_data.iterrows():\n",
    "    if row['Location'] in coordinates:\n",
    "        create_circle_marker(\n",
    "            location=coordinates[row['Location']],\n",
    "            count=row['Count'],\n",
    "            color='red',\n",
    "            radius=10  # Fixed size to highlight\n",
    "        )\n",
    "\n",
    "# Add legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; left: 50px; width: 220px; height: 90px; \n",
    "    border:2px solid grey; z-index:9999; font-size:14px; background-color:white;\n",
    "    \">&nbsp; <b>Legend</b> <br>\n",
    "    &nbsp; COVID-19 tweets &nbsp; <i class=\"fa fa-circle fa-1x\" style=\"color:blue\"></i><br>\n",
    "    &nbsp; Long COVID tweets &nbsp; <i class=\"fa fa-circle fa-1x\" style=\"color:red\"></i>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save map\n",
    "m.save('covid_tweet_map.html')\n",
    "\n",
    "print(\"Map saved as 'covid_tweet_map.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b84dd-8209-42ec-a1ed-ca64e4b02a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
